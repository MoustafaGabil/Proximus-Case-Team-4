{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a PyTorch environment variable to optimize CUDA memory allocation\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd  \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments  # Transformers library for NLP\n",
    "from torch.utils.data import Dataset                                                 # PyTorch Dataset class\n",
    "import os  \n",
    "from google.colab import drive                                                       # For Google Drive integration in Colab\n",
    "import torch                                                                         # PyTorch for deep learning\n",
    "from datasets import load                                                            # Hugging Face datasets library (not used in the code but imported)\n",
    "import torch.nn as nn                                                                # PyTorch neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mount Google Drive to access files stored there\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the file path for the dataset in Google Drive\n",
    "file_path = os.path.join('/content/drive/My Drive/Colab/', \"Phishing_validation_emails - Cleaned.xlsx\")\n",
    "\n",
    "# Read the dataset from the Excel file\n",
    "if not os.path.exists(file_path):\n",
    "    # Raise an error if the file does not exist\n",
    "    raise FileNotFoundError(f\"File not found at: {file_path}\")\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Check for missing values and ensure required columns exist\n",
    "if data.isnull().values.any():\n",
    "    print(\"Warning: Dataset contains missing values. Cleaning data...\")\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "# Ensure the dataset contains necessary columns\n",
    "required_columns = [\"Email Text\", \"Email Type\", \"Word Count\"]\n",
    "for column in required_columns:\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(f\"Missing required column: {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the percentage of data to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage = 0.2        # 0.1 it means 10% of the data will be used for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_train_samples = int(len(data) * percentage)\n",
    "train_data = data.sample(n=num_train_samples, random_state=42)  # Randomly sample training data\n",
    "eval_data = data.drop(data.index[:num_train_samples])  # Use the remaining data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class to handle email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Initialize with data and tokenizer\n",
    "        self.data = data\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Use the EOS token for padding\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Process a single data sample\n",
    "        email_text = self.data.iloc[idx][\"Email Text\"]\n",
    "        email_type = self.data.iloc[idx][\"Email Type\"]\n",
    "        word_count = self.data.iloc[idx][\"Word Count\"]\n",
    "        # Create the input text format\n",
    "        input_text = f\"Type: {email_type}, Words: {word_count}\\nEmail:\\n{email_text}\"\n",
    "        # Tokenize the input text\n",
    "        tokenized_input = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=64)\n",
    "        # Return tokenized data and labels\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokenized_input[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(tokenized_input[\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(tokenized_input[\"input_ids\"])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pre-trained GPT-2 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=1,  # Batch size for training\n",
    "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients for this many steps\n",
    "    eval_accumulation_steps=4,  # Accumulate evaluation results for this many steps\n",
    "    save_steps=10,  # Save checkpoint every 10 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    report_to=\"none\",  # Disable reporting to external systems (e.g., WandB)\n",
    "    fp16=False  # Disable 16-bit floating-point precision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = EmailDataset(train_data)\n",
    "eval_dataset = EmailDataset(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False, **kwargs):\n",
    "    labels = inputs.get(\"labels\")  # Extract labels from inputs\n",
    "    outputs = model(**inputs)  # Forward pass\n",
    "    logits = outputs.get(\"logits\")  # Extract logits\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()  # Define loss function\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))  # Compute loss\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define a metric function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # Extract logits and labels\n",
    "    logits = torch.as_tensor(logits)\n",
    "    labels = torch.as_tensor(labels)\n",
    "    labels = labels[:, 1:].contiguous().view(-1)  # Adjust labels\n",
    "    logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))  # Adjust logits\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()  # Define loss function\n",
    "    loss = loss_fct(logits, labels)  # Compute evaluation loss\n",
    "    return {\"eval_loss\": loss.item()}  # Return loss as a metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Trainer with custom settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics  # Use custom metrics\n",
    ")\n",
    "\n",
    "# Use the custom loss function\n",
    "trainer.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "print(f\"Loss: {results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"/content/drive/My Drive/Colab/trained_model\"\n",
    "model.save_pretrained(output_dir)  # Save the model weights and configuration\n",
    "tokenizer.save_pretrained(output_dir)  # Save the tokenizer configuration\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
